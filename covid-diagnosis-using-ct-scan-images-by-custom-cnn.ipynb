{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vafaknm/covid-diagnosis-using-ct-scan-images-by-custom-cnn?scriptVersionId=93515928\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T12:34:38.685047Z","iopub.execute_input":"2022-02-13T12:34:38.685597Z","iopub.status.idle":"2022-02-13T12:35:01.028857Z","shell.execute_reply.started":"2022-02-13T12:34:38.685504Z","shell.execute_reply":"2022-02-13T12:35:01.02811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading and preprocessing images","metadata":{}},{"cell_type":"markdown","source":"According to the data information, we have 7,593 COVID-19 images and 6,893 normal(non covid) images. First we are going to choose 5000 images from these two groups. random_sample function used for random sampling without replacement.","metadata":{}},{"cell_type":"code","source":"import random\n\nnormal_img_files = os.listdir('../input/large-covid19-ct-slice-dataset/curated_data/curated_data/1NonCOVID')\ncovid_img_files = os.listdir('../input/large-covid19-ct-slice-dataset/curated_data/curated_data/2COVID')\n\nnormal_img_smpls_train = random.sample(normal_img_files, 5000)\ncovid_img_smpls_train = random.sample(covid_img_files, 5000)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T12:35:35.517293Z","iopub.execute_input":"2022-02-13T12:35:35.517547Z","iopub.status.idle":"2022-02-13T12:35:35.539904Z","shell.execute_reply.started":"2022-02-13T12:35:35.51752Z","shell.execute_reply":"2022-02-13T12:35:35.539261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we're going to read the imaging we have chosen using CV2. The shape of images in data is 512 * 512 * 3 and reading them in original shape, take a lot of Ram capacity; So, we can resize them first and append them to arrays later. Then, we create an array for labeling the images.","metadata":{}},{"cell_type":"code","source":"import cv2\n\nnormal_img_train = [] \ncovid_img_train = []\n\nfor img_name in normal_img_smpls_train:\n        image = cv2.imread('../input/large-covid19-ct-slice-dataset/curated_data/curated_data/1NonCOVID/' + img_name, 1)\n        image = cv2.resize(image, dsize=(256,256), interpolation=cv2.INTER_CUBIC)\n        normal_img_train.append(np.array(image))\n        \n\nfor img_name in covid_img_smpls_train:\n        image = cv2.imread('../input/large-covid19-ct-slice-dataset/curated_data/curated_data/2COVID/' + img_name, 1)\n        image = cv2.resize(image, dsize=(256,256), interpolation=cv2.INTER_CUBIC)\n        covid_img_train.append(np.array(image))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T12:35:38.974253Z","iopub.execute_input":"2022-02-13T12:35:38.975125Z","iopub.status.idle":"2022-02-13T12:38:05.977933Z","shell.execute_reply.started":"2022-02-13T12:35:38.975077Z","shell.execute_reply":"2022-02-13T12:38:05.977148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = normal_img_train + covid_img_train\n\ntrain_images_labels = np.concatenate((np.zeros(len(normal_img_train)), np.ones(len(covid_img_train))), axis=None, dtype=np.float32)   #0: normal(non covid) , 1: covid","metadata":{"execution":{"iopub.status.busy":"2022-02-13T12:38:20.193128Z","iopub.execute_input":"2022-02-13T12:38:20.193416Z","iopub.status.idle":"2022-02-13T12:38:20.198635Z","shell.execute_reply.started":"2022-02-13T12:38:20.193386Z","shell.execute_reply":"2022-02-13T12:38:20.197915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training custom vgg16 model","metadata":{}},{"cell_type":"markdown","source":"First we split the whole training images to train and validation sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_images, train_images_labels, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T12:38:28.183313Z","iopub.execute_input":"2022-02-13T12:38:28.18358Z","iopub.status.idle":"2022-02-13T12:38:28.938354Z","shell.execute_reply.started":"2022-02-13T12:38:28.183551Z","shell.execute_reply":"2022-02-13T12:38:28.937614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we build a custom vgg16 model; custom because of 'kernel_initializer', 'activation' functions and amount of neurons 'dropout' we used! Actually, maybe vgg16 exactly use these setting too, I don't know :)))","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import InputLayer, Lambda, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n\ncnn_model = Sequential()\ncnn_model.add(InputLayer(input_shape=(256,256,3)))\ncnn_model.add(Lambda(lambda x: x/255.))   #Normalization\n\ncnn_model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.1))\ncnn_model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(MaxPooling2D((2, 2)))\n\ncnn_model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.1))\ncnn_model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(MaxPooling2D((2, 2)))\n\ncnn_model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.2))\ncnn_model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.2))\ncnn_model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(MaxPooling2D((2, 2)))\n\ncnn_model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.3))\ncnn_model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.3))\ncnn_model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(MaxPooling2D((2, 2)))\n\ncnn_model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.4))\ncnn_model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(Dropout(0.4))\ncnn_model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same'))\ncnn_model.add(MaxPooling2D((2, 2)))\n\ncnn_model.add(Flatten())\ncnn_model.add(Dense(4096, activation=\"relu\"))\ncnn_model.add(Dense(4096, activation=\"relu\"))\ncnn_model.add(Dense(1, activation=\"sigmoid\"))\n\ncnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ncnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T12:38:33.063911Z","iopub.execute_input":"2022-02-13T12:38:33.06434Z","iopub.status.idle":"2022-02-13T12:38:40.459596Z","shell.execute_reply.started":"2022-02-13T12:38:33.064305Z","shell.execute_reply":"2022-02-13T12:38:40.458891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can fit our model. Here, we use two callbacks;\n1. Early Stopping: use validation loss to monitoring the train process for prevent overfitting\n2. Model Checkpoint: to save the model when minimum validation loss occurs at the given path","metadata":{}},{"cell_type":"code","source":"callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n             tf.keras.callbacks.ModelCheckpoint(filepath='../output/kaggle/working/result', monitor='val_loss', mode='min')]\n\nhistory = cnn_model.fit(x = np.asarray(X_train),\n                        y = y_train,\n                        batch_size = 20,\n                        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)],\n                        validation_data = (np.asarray(X_val), y_val),\n                        verbose = 1,\n                        epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T12:38:52.241594Z","iopub.execute_input":"2022-02-13T12:38:52.242325Z","iopub.status.idle":"2022-02-13T13:24:00.956763Z","shell.execute_reply.started":"2022-02-13T12:38:52.242288Z","shell.execute_reply":"2022-02-13T13:24:00.956006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting results of model fitting involvs accuracy, validation accuracy, loss and validation loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13,4))\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('model accuracy')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epoch')\nax[0].legend(['train', 'val'], loc='upper left')\n\nax[1].plot(history.history['loss'][1:])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('model loss')\nax[1].set_ylabel('val_loss')\nax[1].set_xlabel('epoch')\nax[1].legend(['train', 'val'], loc='upper right')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:24:10.046627Z","iopub.execute_input":"2022-02-13T13:24:10.047318Z","iopub.status.idle":"2022-02-13T13:24:18.94019Z","shell.execute_reply.started":"2022-02-13T13:24:10.047283Z","shell.execute_reply":"2022-02-13T13:24:18.939527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the model","metadata":{}},{"cell_type":"markdown","source":"We have chosen 5000 images from two classes for training; now, first we get the images that we did not use them before and select 1500 images from each class for test the model.","metadata":{}},{"cell_type":"code","source":"rest_of_normal_img_files = set(normal_img_files) - set(normal_img_smpls_train)\nrest_of_covid_img_files = set(covid_img_files) - set(covid_img_smpls_train)\n\nnormal_img_smpls_test = random.sample(rest_of_normal_img_files, 1500)\ncovid_img_smpls_test = random.sample(rest_of_covid_img_files, 1500)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:24:34.543136Z","iopub.execute_input":"2022-02-13T13:24:34.543383Z","iopub.status.idle":"2022-02-13T13:24:34.553882Z","shell.execute_reply.started":"2022-02-13T13:24:34.543355Z","shell.execute_reply":"2022-02-13T13:24:34.552759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like training phase, we read the images and resize them","metadata":{}},{"cell_type":"code","source":"normal_img_test = [] \ncovid_img_test = []\n\nfor img_name in normal_img_smpls_test:\n        image = cv2.imread('../input/large-covid19-ct-slice-dataset/curated_data/curated_data/1NonCOVID/' + img_name, 1)\n        image = cv2.resize(image, dsize=(256,256), interpolation=cv2.INTER_CUBIC)\n        normal_img_test.append(np.array(image))\n        \n\nfor img_name in covid_img_smpls_test:\n        image = cv2.imread('../input/large-covid19-ct-slice-dataset/curated_data/curated_data/2COVID/' + img_name, 1)\n        image = cv2.resize(image, dsize=(256,256), interpolation=cv2.INTER_CUBIC)\n        covid_img_test.append(np.array(image))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:24:38.691228Z","iopub.execute_input":"2022-02-13T13:24:38.691483Z","iopub.status.idle":"2022-02-13T13:25:18.797453Z","shell.execute_reply.started":"2022-02-13T13:24:38.691455Z","shell.execute_reply":"2022-02-13T13:25:18.796629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = normal_img_test + covid_img_test\n\ntest_images_labels = np.concatenate((np.zeros(len(normal_img_test)), np.ones(len(normal_img_test))), axis=None, dtype=np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:25:22.795114Z","iopub.execute_input":"2022-02-13T13:25:22.795381Z","iopub.status.idle":"2022-02-13T13:25:22.800129Z","shell.execute_reply.started":"2022-02-13T13:25:22.795353Z","shell.execute_reply":"2022-02-13T13:25:22.799435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'evaluate' method returns the loss value & metrics values (accuracy in this project) for the model.","metadata":{}},{"cell_type":"code","source":"cnn_model.evaluate(x = np.asarray(test_images),\n                   y = test_images_labels,\n                   verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:25:30.629446Z","iopub.execute_input":"2022-02-13T13:25:30.629884Z","iopub.status.idle":"2022-02-13T13:25:45.91129Z","shell.execute_reply.started":"2022-02-13T13:25:30.62985Z","shell.execute_reply":"2022-02-13T13:25:45.910519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In fact, loss and accuracy values alone can not describe our model! So we use 'classification_report' function from sklearn to get Precision, Recall & F1 Scores which express the model better.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.researchgate.net/profile/Nittaya-Kerdprasop/publication/329526806/figure/fig1/AS:745215891623936@1554684722023/Example-of-confusion-matrix-True-Positive-TP-The-number-of-instances-that-a-model.ppm)","metadata":{}},{"cell_type":"markdown","source":"* **Precision:**\n\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question 'precision' answers is: Of all patients that labeled as covid, how many actually covid patient?\n\nPrecision = TP / TP+FP","metadata":{}},{"cell_type":"markdown","source":"* **Recall:**\n\nRecall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question 'recall' answers is: Of all the patients that truly have covid, how many did we label?\n\nRecall = TP / TP+FN","metadata":{}},{"cell_type":"markdown","source":"* **F1 score:**\n\nF1 Score is the weighted average of 'precision' and 'recall'. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy\n\nF1 Score = 2*(Recall * Precision) / (Recall + Precision)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel_prd = cnn_model.predict(x = np.asarray(test_images))\nfor i in range(len(model_prd)):\n    if model_prd[i] >= 0.5:\n        model_prd[i] = 1\n    elif model_prd[i] < 0.5:\n        model_prd[i] = 0\n\nclasses = ['Class Normal', 'Class Covid']\nprint(classification_report(y_true = test_images_labels,\n                      y_pred = model_prd,\n                      target_names = classes))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:25:53.364629Z","iopub.execute_input":"2022-02-13T13:25:53.365167Z","iopub.status.idle":"2022-02-13T13:26:01.311085Z","shell.execute_reply.started":"2022-02-13T13:25:53.365129Z","shell.execute_reply":"2022-02-13T13:26:01.310371Z"},"trusted":true},"execution_count":null,"outputs":[]}]}